{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Openimages-ResNet-TransferLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9PZmdGTBzDz"
      },
      "source": [
        "#I install the openimages dataset package using pip\n",
        "!pip install openimages\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH0UKrtPISgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9769fa-70da-4f08-9ffb-19daf08396e8"
      },
      "source": [
        "#I use this console command to download the images.\n",
        "!oi_download_images --csv_dir /content/csv_dir --base_dir /content/base_dir --labels Coffee Dog --limit 600\n",
        "# source: https://towardsdatascience.com/how-to-easily-download-googles-open-images-dataset-for-your-ai-apps-db552a82fc6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1194041344 bytes == 0x58e54000 @  0x7fe9de5cf1e7 0x59211c 0x598dce 0x50a1cc 0x50beb4 0x507be4 0x588c8b 0x4b6271 0x55dc5d 0x50bddb 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x508ec2 0x594a01 0x59fd0e 0x50d256 0x507be4 0x588e5c 0x59fd0e 0x50d256 0x507be4 0x509900 0x50a2fd 0x50cc96 0x5095c8 0x50a2fd 0x50beb4\n",
            "2020-11-09  18:38:37 INFO NumExpr defaulting to 2 threads.\n",
            "2020-11-09  18:38:40 INFO Downloading 557 train images for class 'coffee'\n",
            "100% 557/557 [00:08<00:00, 63.47it/s]\n",
            "2020-11-09  18:38:49 INFO Downloading 600 train images for class 'dog'\n",
            " 99% 593/600 [00:09<00:00, 58.97it/s]2020-11-09  18:38:58 WARNING Connection pool is full, discarding connection: open-images-dataset.s3.amazonaws.com\n",
            "100% 600/600 [00:09<00:00, 63.79it/s]\n",
            "2020-11-09  18:38:59 INFO Downloading 43 validation images for class 'coffee'\n",
            "100% 43/43 [00:01<00:00, 41.92it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWxDDX_8S4yU"
      },
      "source": [
        "#Here I import some functions, packages\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPyt1ZlZMMfE"
      },
      "source": [
        "#Some constants for the data to numpy conversion\n",
        "TARGET_X=224  #ResNet wants 224*224 images as input\n",
        "TARGET_Y=224\n",
        "PATH='/content/base_dir'\n",
        "\n",
        "#Empty lists into which i will put the data\n",
        "labelstrain=[]\n",
        "labelsval=[]\n",
        "labelstest=[]\n",
        "imagestrain=[]\n",
        "imagesval=[]\n",
        "imagestest=[]\n",
        "\n",
        "\n",
        "#This is a generic data reading loop, it can work for multiple labels as well\n",
        "#The task was to have 400 images as training data, 100-100 as validation and test from each label\n",
        "for label, dir in enumerate(os.listdir(PATH)): #The label value will change depending on which type of image's directory we are in.\n",
        "  innerpath=os.path.join(PATH,dir)\n",
        "  for p in os.listdir(innerpath):\n",
        "    finalpath=os.path.join(innerpath, p)\n",
        "    for idx, file in enumerate(os.listdir(finalpath)):\n",
        "      if idx<400: #training data\n",
        "        imagestrain.append(image.img_to_array(image.load_img(os.path.join(finalpath, file), target_size=(TARGET_X, TARGET_Y)))) #Here I load the images with the correct size, convert it to numpy arrays, then append it to the correct list\n",
        "        labelstrain.append(label) #I also append the labelarrays with the correct labels\n",
        "      elif 400<=idx and idx<500:#validation data\n",
        "        imagesval.append(image.img_to_array(image.load_img(os.path.join(finalpath, file), target_size=(TARGET_X, TARGET_Y))))\n",
        "        labelsval.append(label)\n",
        "      else:#The remaining 100 is the test data, because i loaded 600 of each label\n",
        "        imagestest.append(image.img_to_array(image.load_img(os.path.join(finalpath, file), target_size=(TARGET_X, TARGET_Y))))\n",
        "        labelstest.append(label)\n",
        "\n",
        "\n",
        "\n",
        "#I make numpy arrays from the normal lists, because Keras likes numpy arrays.\n",
        "labelstrain=np.asarray(labelstrain)\n",
        "labelsval=np.asarray(labelsval)\n",
        "labelstest=np.asarray(labelstest)\n",
        "imagestrain=np.asarray(imagestrain)\n",
        "imagesval=np.asarray(imagesval)\n",
        "imagestest=np.asarray(imagestest)\n",
        "\n",
        "#I use numpy's random permutation to shuffle both the images and labels\n",
        "randpermtrain = np.random.permutation(len(labelstrain)) #This is a permutation of 400 length\n",
        "randpermtest = np.random.permutation(len(labelstest)) #This is a permutation of 100 length\n",
        "\n",
        "imagestrain=imagestrain[randpermtrain, :, :]\n",
        "labelstrain=labelstrain[randpermtrain]\n",
        "imagesval=imagesval[randpermtest,:, :]\n",
        "labelsval=labelsval[randpermtest]\n",
        "imagestest=imagestest[randpermtest,:, :]\n",
        "labelstest=labelstest[randpermtest]\n",
        "\n",
        "\n",
        "\n",
        "#I preprocess the input using the pretrained model's builtin function\n",
        "imagestrain=preprocess_input(imagestrain)\n",
        "imagesval=preprocess_input(imagesval)\n",
        "imagestest=preprocess_input(imagestest)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuY6RLLfJcqN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c03b90-3543-4b9a-a6da-b44153c45c43"
      },
      "source": [
        "#I load the base_model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False) #The original model has a softmax on the top layer, but i want a sigmoid, so i dont include its top layers.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ0vL-4VJdH3"
      },
      "source": [
        "#Some more necessary imports\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.callbacks import EarlyStopping, CSVLogger\n",
        "from keras.activations import sigmoid\n",
        "from keras import Model\n",
        "\n",
        "#I put some layers to the top of the model, using Keras' Functional API\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x) #This layer was in the original model, but it wasn't included, because of the include_top=False parameter.\n",
        "x = Dense(1024, activation='relu')(x) #To feed the last sigmoid layer, i put a Dense layer with more neurons, with a relu.\n",
        "output = Dense(1, activation='sigmoid')(x) #I put a final layer on the model, so it can have a sigmoid activation function for binary classification.\n",
        "model=Model(base_model.input, output) #I construct the model, giving its inputs and outputs to the constructor of the Model class\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP_o2cu6TVkq"
      },
      "source": [
        "#I freeze all layers, except the last 2 Dense ones.\n",
        "#This is for homework task 1.\n",
        "for i, layer in enumerate(model.layers):\n",
        "  if type(layer) is not Dense:\n",
        "    layer.trainable=False\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvUuvGgRTYIu"
      },
      "source": [
        "#I compile the model, using binary_crossentropy for binary prediction.\n",
        "model.compile('adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwDqM9e7TYdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456a27fe-6198-4411-8b7d-d03262f70f95"
      },
      "source": [
        "#I train and test the model here, with EarlyStopping to avoid overfitting.\n",
        "#I use Keras' CSVLogger to make logs of the training\n",
        "model.fit(imagestrain, labelstrain, validation_data=(imagesval, labelsval), epochs=10, batch_size=16, callbacks=[EarlyStopping(patience=2, monitor='val_loss', restore_best_weights=True), CSVLogger('training.log', separator=';', append=True)])\n",
        "_, accuracy=model.evaluate(imagestest, labelstest)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 138s 3s/step - loss: 0.0965 - accuracy: 0.9675 - val_loss: 3.5433e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 136s 3s/step - loss: 0.0055 - accuracy: 0.9987 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 137s 3s/step - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.0208 - val_accuracy: 0.9900\n",
            "7/7 [==============================] - 23s 3s/step - loss: 0.0574 - accuracy: 0.9800\n",
            "0.9800000190734863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiC-YqFSVrQ-"
      },
      "source": [
        "#I freeze all layers, except the Convolutional block, and the layers after that.\n",
        "#This is for homework task 2.\n",
        "for i, layer in enumerate(model.layers):\n",
        "  if i <= 163:\n",
        "    layer.trainable=False\n",
        "  else:\n",
        "    layer.trainable=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWj4CTyBX2Le",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc3e1d8-b4a6-46dc-ea82-5895a048ae85"
      },
      "source": [
        "#I continue the training of the re-trained pretrained model.\n",
        "model.fit(imagestrain, labelstrain, validation_data=(imagesval, labelsval), epochs=10, batch_size=16, callbacks=[EarlyStopping(patience=2, monitor='val_loss', restore_best_weights=True), CSVLogger('training.log', separator=';', append=True)])\n",
        "_, accuracy=model.evaluate(imagestest, labelstest)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 136s 3s/step - loss: 0.0018 - accuracy: 0.9987 - val_loss: 3.2886e-06 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 138s 3s/step - loss: 0.0334 - accuracy: 0.9912 - val_loss: 0.0398 - val_accuracy: 0.9950\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 136s 3s/step - loss: 0.0703 - accuracy: 0.9950 - val_loss: 2.3732e-06 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 135s 3s/step - loss: 4.9907e-04 - accuracy: 1.0000 - val_loss: 1.4007e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 135s 3s/step - loss: 1.4319e-05 - accuracy: 1.0000 - val_loss: 3.6723e-05 - val_accuracy: 1.0000\n",
            "7/7 [==============================] - 23s 3s/step - loss: 0.0025 - accuracy: 1.0000\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPKKRDFFTJ1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73776859-9a35-4c6e-ad70-34b49fdf68b0"
      },
      "source": [
        "#I make some predictions for fun\n",
        "\n",
        "!oi_download_images --csv_dir /content/csv_dir --base_dir /content/preds_dir --labels Coffee Dog --limit 10\n",
        "\n",
        "#The two directories for predictions\n",
        "img1_dir = '/content/preds_dir/dog/images'\n",
        "img0_dir = '/content/preds_dir/coffee/images'\n",
        "\n",
        "#List of images\n",
        "imgarr = []\n",
        "\n",
        "#I read in the images, like i did before\n",
        "for i, imag in enumerate(os.listdir(img0_dir)):\n",
        "    img_path=os.path.join(img0_dir, imag)\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    x = image.img_to_array(img)\n",
        "    imgarr.append(x)\n",
        "\n",
        "for i, imag in enumerate(os.listdir(img1_dir)):\n",
        "    img_path=os.path.join(img1_dir, imag)\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    x = image.img_to_array(img)\n",
        "    imgarr.append(x)\n",
        "\n",
        "\n",
        "imgarr = preprocess_input(np.asarray(imgarr))\n",
        "\n",
        "print(imgarr.shape)\n",
        "\n",
        "#Here I make the model do some predictions\n",
        "for idx, img in enumerate(imgarr):\n",
        "  preds = model.predict(np.expand_dims(img, axis=0))\n",
        "  print('Predicted class:')\n",
        "  print(round(preds[0][0]))\n",
        "  print('True class:')\n",
        "  if idx<10:\n",
        "    print(0)\n",
        "  else:\n",
        "    print(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-09  18:59:16 INFO NumExpr defaulting to 2 threads.\n",
            "2020-11-09  18:59:19 INFO Downloading 10 train images for class 'coffee'\n",
            "100% 10/10 [00:00<00:00, 22.44it/s]\n",
            "2020-11-09  18:59:19 INFO Downloading 10 train images for class 'dog'\n",
            "100% 10/10 [00:00<00:00, 25.33it/s]\n",
            "(20, 224, 224, 3)\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "0.0\n",
            "True class:\n",
            "0\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n",
            "Predicted class:\n",
            "1.0\n",
            "True class:\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}